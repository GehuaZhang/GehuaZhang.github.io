<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Machine Learning Models - Gehua Zhang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Machine Learning Models.">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Models">
<meta property="og:url" content="https://gehuazhang.github.io/articles/machinelearningnote1/index.html">
<meta property="og:site_name" content="Gehua Zhang&#39;s Blog">
<meta property="og:description" content="Machine Learning Models.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://gehuazhang.github.io/articles/machinelearningnote1/LDA.png">
<meta property="og:image" content="https://gehuazhang.github.io/articles/machinelearningnote1/QDA.png">
<meta property="og:updated_time" content="2019-11-23T02:17:24.687Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Models">
<meta name="twitter:description" content="Machine Learning Models.">
<meta name="twitter:image" content="https://gehuazhang.github.io/articles/machinelearningnote1/LDA.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel="stylesheet" type="text/css">
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo logo-text" href="/">Gehua Zhang&#39;s Blog</a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories">Category</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://gehuazhang.github.io"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-machinelearningnote1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Machine Learning Models
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/articles/machinelearningnote1/" class="article-date">
  <time datetime="2019-11-22T21:22:10.000Z" itemprop="datePublished">2019-11-22</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/programming/">Programming</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Machine-Learning-Models"><a href="#Machine-Learning-Models" class="headerlink" title="Machine Learning Models."></a>Machine Learning Models.</h4><a id="more"></a>
<hr>
<h2 id="Markov-Chains"><a href="#Markov-Chains" class="headerlink" title="Markov Chains"></a>Markov Chains</h2><h3 id="1-Markov-Chains"><a href="#1-Markov-Chains" class="headerlink" title="1. Markov Chains"></a>1. Markov Chains</h3><ul>
<li><p><em><strong>Properties</strong></em></p>
<ul>
<li><p><strong>Irreducible:</strong> We can access any states from any states. $Pr(x_i=s|x_j=k)&gt;0$</p>
</li>
<li><p><strong>Aperiodic:</strong> Transition matrix has non-zero diagonal.$Pr(x_i=s|x_{i-1}=s)&gt;0$</p>
<ul>
<li><strong>Stationary:</strong> $P_{s-&gt;t}$ is independent of n.</li>
</ul>
</li>
<li><p><strong>Conditional Independent:</strong> $Pr(x,y|z)=Pr(x|z)Pr(y|z)$</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-Hidden-Markov-Model"><a href="#2-Hidden-Markov-Model" class="headerlink" title="2. Hidden Markov Model"></a>2. Hidden Markov Model</h3><ul>
<li><p><em><strong>Structure</strong></em></p>
<p>Hidden Variable -(Trasition Matrix)-&gt; Hidden Variable -â€¦-&gt; Hidden Variable</p>
<p>Emission Probability = $p(y|x)$</p>
</li>
<li><p><em><strong>Filtering Problem:</strong></em> Estimate Hidden Variable</p>
<p>With emission matrix known:</p>
<p>$$Q(k|x_n) = \frac {Pr(X_n|Z_n,X_{n-1})Pr(Z_n|X_{n-1})}{ \sum Pr(X_n|Z_n,X_{n-1}) Pr(Z_n|X_{n-1})}$$</p>
</li>
</ul>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h3 id="3-K-means"><a href="#3-K-means" class="headerlink" title="3. K-means"></a>3. K-means</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li><p>Pick K and randomly assign K different centroids ($c_j, j=1..K$).</p>
</li>
<li><p>Repeat until convergence: Find distance of each point ($X_i,i=1..n$) and each centroids $c_j$, use it to find the closest centroids of every point, assign this point a new cluster ($j$). After all points are updated, calculate the new centroid of each cluster.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><em><strong>Pseudo-Code:</strong></em> </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Use Euclidean distance for distance measure</span><br><span class="line">Input: K, X=[x_1,x_2,..,x_n]</span><br><span class="line">1. Random location for c_j = [c_1,c_2,..,c_K]</span><br><span class="line">2. While c_j is changed between interations:</span><br><span class="line">	for i in [1 ... n]:</span><br><span class="line">		nearest_j = argmin Distance(x_i,c_j)</span><br><span class="line">		assign xi to nearest_j</span><br><span class="line">	for j in [1 ... K]:</span><br><span class="line">		c_j = mean(x_i that assigned to j)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li><p>Stuck to local minimum</p>
</li>
<li><p>Each trial may have different result. Initialization is important.</p>
</li>
</ul>
</li>
</ul>
<h3 id="4-K-medoids"><a href="#4-K-medoids" class="headerlink" title="4. K-medoids"></a>4. K-medoids</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li><p>Randomly assign each point to a cluster from $1$ to $K$.</p>
</li>
<li><p>Repeat until convergence: For each cluster, find a point $x_i$ which minimize the total pairwise distance $z_j = \text{argmin} \sum D(x_i, x_{i^*})$. Assign each points to its closest cluster $j$ from calculating $D(x_i,z_j)$</p>
</li>
</ul>
</li>
<li><p><em><strong>Pseudo-Code:</strong></em> </p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input: K, X=[x_1,x_2,..,x_n]</span><br><span class="line">1. Random cluster for each point X</span><br><span class="line">2. While c_j is changed between interations:</span><br><span class="line">	for j in K:</span><br><span class="line">		x_ij = [x_i is assined j]</span><br><span class="line">		for x_i in x_ij:</span><br><span class="line">			x_i* = x_ij exclude x_i</span><br><span class="line">			c_j = argmin Distance(x_i, x_i*)</span><br><span class="line">	for i in n:</span><br><span class="line">		nearest_j = argmin Distance(x_i,c_j)</span><br><span class="line">		assign nearest_j to x_i</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li>Same as K-means, except that centroid is required to be one of the observations.</li>
<li>Deal with categorical variables.</li>
</ul>
</li>
</ul>
<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><h3 id="5-K-NN"><a href="#5-K-NN" class="headerlink" title="5. K-NN"></a>5. K-NN</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li>Find the K nearest neighbors around a new point, assign the majority vote of K neighbors to that point.</li>
</ul>
</li>
<li><p><em><strong>Pseudo-Code:</strong></em> </p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: K, X_train(1,2,..,n), X_predict</span><br><span class="line">k_nn = sort(Distance(X_predict,X_train),&quot;ascending&quot;)[1:K,]</span><br><span class="line">Y_predict = majority_class(knn)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li>Can fail in high dimensions, because it becomes difficult to gather K observations close to a target point.</li>
<li>Large $K$, smoother decision boundary, error $\downarrow$, variance $\uparrow$. Bias &amp; Variance tradeoff.</li>
</ul>
</li>
</ul>
<h3 id="6-Bayesian-Classifer"><a href="#6-Bayesian-Classifer" class="headerlink" title="6. Bayesian Classifer"></a>6. Bayesian Classifer</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li>Want to find a function of $x$ that outputs the value of $y$ in a most reliable way, this is to say, find $y=f(x) = \text{argmax } P(y|x)$.</li>
<li>Using Bayesian formula: $P(y|x) \propto P(y)P(x|y)$, ignore the scale term in denominator.</li>
<li><strong>Direct way:</strong> $P(y)=\frac{N_{happen}}{N_{total}}$ is a prior from historical data (could directly use portion), then find $P(x|y)$ by seperating the data and using maximum likelihood.</li>
</ul>
</li>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li>Logit and LDA are also Bayesian classifers, here just list the most direct way to apply bayesian classifer, ie, using portion probability.</li>
</ul>
</li>
</ul>
<h3 id="7-Logit"><a href="#7-Logit" class="headerlink" title="7. Logit"></a>7. Logit</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li>Using maximize likelihood to find $\beta_i$ï¼š</li>
</ul>
</li>
</ul>
<p>$$ \log [\frac{P(Y=1|X)}{P(Y=0|X)}]=\beta_0+\beta_1x_1+\beta_2x_2+â€¦+\beta_px_p $$</p>
<p>$$\prod_{i=1}^{n} P(Y=y_i|X=x_i)=\prod_{y_i=1} P(Y=1|X=x_i)\prod_{y_i=0} P(Y=0|X=x_i)$$</p>
<p>$$=\prod_{y_i=1}\frac{e^{\beta_0+\beta_1x_{i1}+..+\beta_px_{ip}}}{1+e^{\beta_0+\beta_1x_{i1}+..+\beta_px_{ip}}}\prod_{y_i=0}\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+..+\beta_px_{ip}}}$$</p>
<ul>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li><p>Difficult to extend to more than 2 categories. Assumed Y is a binary variable. If more than 2 categories, use baseline.</p>
</li>
<li><p>The coefficients become unstable when there is collinearity. Affects the convergence of the fitting algorithm.</p>
</li>
<li><p>When the classes are well separated, the coefficients become unstable. This is always the case when $p \ge n âˆ’ 1$.</p>
</li>
</ul>
</li>
</ul>
<h3 id="8-LDA-Linear-Discriminant-Analysis"><a href="#8-LDA-Linear-Discriminant-Analysis" class="headerlink" title="8. LDA-Linear Discriminant Analysis"></a>8. LDA-Linear Discriminant Analysis</h3><p><img src="/articles/machinelearningnote1/LDA.png" alt="LDA"></p>
<ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li><p>Still apply Bayesian idea, $P(y|x) \propto P(y)P(x|y)$, needs to find $P(x|y)$. Split data into $k$ categories, we model each of them $P(x|y=k)$ as a Normal Distribution, hence $P(y|x)$ becomes a Multivariate Normal Distribution ($k$ normal distribution shares a same covariance. *left graph). We need to maximize $P(x|y)$. Its <a href="http://cs229.stanford.edu/section/gaussians.pdf" target="_blank" rel="noopener">PDF</a> contains three parameters: $\mu_k, \Sigma, \pi_k$.</p>
</li>
<li><p>$\mu_k$ is the center of each class $k$, $\Sigma$ is the covariance martix, estimated by vectors of deviations $(x_1-\mu_{y_1}),(x_2-\mu_{y_2}),â€¦(x_n-\mu_{y_n})$. $\pi_k$ is the fraction of training samples of class k.</p>
</li>
</ul>
</li>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li><p>LDA assumes common covariance to all categories ($\Sigma$ is the same for all $k$ categories).</p>
</li>
<li><p>LDA will have a linear decision boundary (*right graph).</p>
</li>
</ul>
</li>
</ul>
<h3 id="9-QDA-Quadratic-Discriminant-Analysis"><a href="#9-QDA-Quadratic-Discriminant-Analysis" class="headerlink" title="9. QDA-Quadratic Discriminant Analysis"></a>9. QDA-Quadratic Discriminant Analysis</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
<ul>
<li>Similar to LDA except we wonâ€™t assume a same covariance. We assume each category has its own covariance matrix $\Sigma_k$.</li>
</ul>
</li>
<li><p><em><strong>Feature</strong></em></p>
<ul>
<li>The decision boundary is quadratic.</li>
</ul>
</li>
</ul>
<p><img src="/articles/machinelearningnote1/QDA.png" alt="QDA"></p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><ul>
<li><p><em><strong>Algorithm</strong></em></p>
</li>
<li><p><em><strong>Pseudo-Code:</strong></em> </p>
</li>
<li><p><em><strong>Feature</strong></em></p>
</li>
</ul>
<h2 id="Unsupervised"><a href="#Unsupervised" class="headerlink" title="Unsupervised"></a>Unsupervised</h2>
      
    </div>
    
    
      <footer class="article-footer">
        
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/articles/machinelearningnote2/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          Machine Learning Validation
        
      </div>
    </a>
  
  
    <a href="/articles/stocstccalculus/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Derive Black-Scholes Formula&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>




<div class="share_addthis">
  <div class="sharing addthis_toolbox share">
    <a class="addthis_button_facebook_like"></a>
    <a class="addthis_button_tweet"></a>
    <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-560c64c35486b3d4" async="async"></script>
</div>




<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>


</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Gehua Zhang&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    
<script>
  var disqus_shortname = 'gehuazhangblog';
  
  var disqus_url = 'https://gehuazhang.github.io/articles/machinelearningnote1/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>